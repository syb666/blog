# 单线程爬取斗图网的图片
```
# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
from urllib.request import urlretrieve
import lxml,urllib
from lxml import etree
import os
#基本url
BASE_URL = 'https://www.doutula.com/photo/list/?page='
#获取每一页的url
PAGE_URLS = []
headers = {
    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36'
}
#下载每一页的图片
def get_down_image(url,index):
    filename = url.split('/')[-1]
    # print(index)
    os.makedirs('./images/page{}_image/'.format(index), exist_ok=True)#创建一个文件夹
    filename = filename.split('!')[-2]
    path = os.path.join('images/page{}_image'.format(index),filename)
    urlretrieve(url,filename=path)#下载图片

#获取每一个图片的url
def get_image_urls(url,index):
    response = requests.get(url,headers=headers)
    context = response.text
    html = etree.HTML(context)
    # soup = BeautifulSoup(context,'lxml')
    image_urls = html.xpath("//div[@class='page-content text-center']//img/@data-original")
    # print(context)
    for image_url in image_urls:
        # print(image_url)
        get_down_image(image_url,index)

#获取每一页url
def get_urls_list():
    for x in range(5):
        url = BASE_URL+str(x)
        PAGE_URLS.append(url)
    return PAGE_URLS

def main():
    urls = get_urls_list()
    for index,url in enumerate(urls):
        get_image_urls(url,index)
        # break

if __name__ == '__main__':
    main()
```
# 多线程爬取斗图网
```
# encoding: utf-8

from urllib.request import urlretrieve
import threading
from bs4 import BeautifulSoup
import requests
import os
import time

# 表情链接列表
FACE_URL_LIST = []
# 页面链接列表
PAGE_URL_LIST = []
# 构建869个页面的链接
BASE_PAGE_URL = 'https://www.doutula.com/photo/list/?page='
for x in range(5):
    url = BASE_PAGE_URL + str(x)
    PAGE_URL_LIST.append(url)
# 初始化锁
gLock = threading.Lock()


# 生产者，负责从每个页面中提取表情的url
class Producer(threading.Thread):
    def run(self):
        while len(PAGE_URL_LIST) > 0:
            # 在访问PAGE_URL_LIST的时候，要使用锁机制
            gLock.acquire()
            page_url = PAGE_URL_LIST.pop()
            # 使用完后要及时把锁给释放，方便其他线程使用
            gLock.release()
            response = requests.get(page_url)
            soup = BeautifulSoup(response.content, 'lxml')
            img_list = soup.find_all('img', attrs={'class': 'img-responsive lazy image_dta'})
            gLock.acquire()
            for img in img_list:
                src = img['data-original']
                src = src.split('!')[-2]
                if not src.startswith('http'):
                    src = 'http:' + src
                # 把提取到的表情url，添加到FACE_URL_LIST中
                FACE_URL_LIST.append(src)
            gLock.release()
            # time.sleep(0.5)
#

# 消费者，负责从FACE_URL_LIST提取表情链接，然后下载
class Consumer(threading.Thread):
    def run(self):
        # print('%s is running' % threading.current_thread)
        while True:
            # 上锁
            gLock.acquire()
            if len(FACE_URL_LIST) == 0:
                # 不管什么情况，都要释放锁
                gLock.release()
                continue
            else:
                # 从FACE_URL_LIST中提取数据
                face_url = FACE_URL_LIST.pop()
                # index = face_url.split("=")[-1]
                # print(face_url)
                gLock.release()
                filename = face_url.split('/')[-1]
                os.makedirs('./image', exist_ok=True)  # 创建一个文件夹
                path = os.path.join('image', filename)
                urlretrieve(face_url, filename=path)


if __name__ == '__main__':
    # 2个生产者线程，去从页面中爬取表情链接
    start = time.time()
    for x in range(3):
        Producer().start()

    # 5个消费者线程，去从FACE_URL_LIST中提取下载链接，然后下载
    for x in range(5):
        Consumer().start()
    end = time.time()
    print(end-start)
```
